{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch tensorboard","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:21:06.015697Z","iopub.execute_input":"2024-11-13T18:21:06.016116Z","iopub.status.idle":"2024-11-13T18:21:17.609419Z","shell.execute_reply.started":"2024-11-13T18:21:06.016066Z","shell.execute_reply":"2024-11-13T18:21:17.608445Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.16.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.62.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.6)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (70.0.0)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:21:17.611682Z","iopub.execute_input":"2024-11-13T18:21:17.612476Z","iopub.status.idle":"2024-11-13T18:21:29.678271Z","shell.execute_reply.started":"2024-11-13T18:21:17.612424Z","shell.execute_reply":"2024-11-13T18:21:29.677091Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.1.1)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:21:29.679741Z","iopub.execute_input":"2024-11-13T18:21:29.680090Z","iopub.status.idle":"2024-11-13T18:22:24.260097Z","shell.execute_reply.started":"2024-11-13T18:21:29.680056Z","shell.execute_reply":"2024-11-13T18:22:24.259158Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n  Cloning https://github.com/huggingface/trl (to revision a3c5b7178ac4f65569975efadc97db2f3749c65e) to /tmp/pip-req-build-me37sdb1\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl /tmp/pip-req-build-me37sdb1\n  Running command git rev-parse -q --verify 'sha^a3c5b7178ac4f65569975efadc97db2f3749c65e'\n  Running command git fetch -q https://github.com/huggingface/trl a3c5b7178ac4f65569975efadc97db2f3749c65e\n  Running command git checkout -q a3c5b7178ac4f65569975efadc97db2f3749c65e\n  Resolved https://github.com/huggingface/trl to commit a3c5b7178ac4f65569975efadc97db2f3749c65e\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.11.dev0) (2.4.0)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.11.dev0) (4.46.2)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.11.dev0) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.7.11.dev0) (1.1.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.7.11.dev0) (3.1.0)\nRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.11.dev0) (0.8.14)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.11.dev0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.11.dev0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.11.dev0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.11.dev0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.11.dev0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.11.dev0) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (0.25.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (4.66.4)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11.dev0) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11.dev0) (13.7.1)\nRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11.dev0) (1.7.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.7.11.dev0) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.11.dev0) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.11.dev0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.11.dev0) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.11.dev0) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.11.dev0) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.11.dev0) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.11.dev0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.11.dev0) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.11.dev0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.11.dev0) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.11.dev0) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.11.dev0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.7.11.dev0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.11.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.11.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.11.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.11.dev0) (2024.8.30)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11.dev0) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.7.11.dev0) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.11.dev0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.11.dev0) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.11.dev0) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.7.11.dev0) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11.dev0) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.7.11.dev0) (1.16.0)\nCollecting git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f\n  Cloning https://github.com/huggingface/peft (to revision 4a1559582281fc3c9283892caea8ccef1d6f5a4f) to /tmp/pip-req-build-jnh1waib\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-jnh1waib\n  Running command git rev-parse -q --verify 'sha^4a1559582281fc3c9283892caea8ccef1d6f5a4f'\n  Running command git fetch -q https://github.com/huggingface/peft 4a1559582281fc3c9283892caea8ccef1d6f5a4f\n  Running command git checkout -q 4a1559582281fc3c9283892caea8ccef1d6f5a4f\n  Resolved https://github.com/huggingface/peft to commit 4a1559582281fc3c9283892caea8ccef1d6f5a4f\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.2.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.2.dev0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.7.2.dev0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.7.2.dev0) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.2.dev0) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.7.2.dev0) (4.46.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.7.2.dev0) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.2.dev0) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.7.2.dev0) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.7.2.dev0) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.7.2.dev0) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.7.2.dev0) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.7.2.dev0) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from huggingface_hub import login\n \nlogin(\n  token=\"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\", \n  add_to_git_credential=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:24.263065Z","iopub.execute_input":"2024-11-13T18:22:24.263792Z","iopub.status.idle":"2024-11-13T18:22:25.241780Z","shell.execute_reply.started":"2024-11-13T18:22:24.263753Z","shell.execute_reply":"2024-11-13T18:22:25.240868Z"}},"outputs":[{"name":"stdout","text":"Token is valid (permission: fineGrained).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    Trainer, \n    TrainingArguments, \n    pipeline, \n    BitsAndBytesConfig,\n    HfArgumentParser, \n    logging\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_int8_training\nimport numpy as np\nimport evaluate\nimport copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:25.243008Z","iopub.execute_input":"2024-11-13T18:22:25.243325Z","iopub.status.idle":"2024-11-13T18:22:32.699057Z","shell.execute_reply.started":"2024-11-13T18:22:25.243293Z","shell.execute_reply":"2024-11-13T18:22:32.698301Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"use_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:32.700182Z","iopub.execute_input":"2024-11-13T18:22:32.700781Z","iopub.status.idle":"2024-11-13T18:22:32.705014Z","shell.execute_reply.started":"2024-11-13T18:22:32.700745Z","shell.execute_reply":"2024-11-13T18:22:32.704126Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n   load_in_4bit=use_4bit,\n   bnb_4bit_quant_type=bnb_4bit_quant_type,\n   bnb_4bit_compute_dtype=compute_dtype,\n   bnb_4bit_use_double_quant=use_nested_quant,\n)\n\nif compute_dtype == torch.float16 and use_4bit:\n   major, _ = torch.cuda.get_device_capability()\n   if major >= 8:\n       print(\"=\" * 80)\n       print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n       print(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:32.706337Z","iopub.execute_input":"2024-11-13T18:22:32.707099Z","iopub.status.idle":"2024-11-13T18:22:32.767082Z","shell.execute_reply.started":"2024-11-13T18:22:32.707053Z","shell.execute_reply":"2024-11-13T18:22:32.766127Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Step 1: Load model and tokenizer\nmodel_name = \"google/gemma-2-2b-it\"\ntokenizer = AutoTokenizer.from_pretrained(model_name , trust_remote_code=True)\n\ntokenizer.pad_token = tokenizer.eos_token \nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map={\"\": 0}\n)\nmodel = prepare_model_for_int8_training(model)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:32.768221Z","iopub.execute_input":"2024-11-13T18:22:32.768500Z","iopub.status.idle":"2024-11-13T18:22:40.940050Z","shell.execute_reply.started":"2024-11-13T18:22:32.768469Z","shell.execute_reply":"2024-11-13T18:22:40.939088Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20bc297e06624775847d9dd6495eb880"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:143: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Step 2: Apply LoRA configuration and freeze base model parameters\nlora_config = LoraConfig(r=64, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.1)\nmodel = get_peft_model(model, lora_config)\n\n\n# Freeze the base model parameters except lora layers; only LoRA parameters will be trainable\nfor name, param in model.named_parameters():\n    if \"lora_\" not in name:\n        param.requires_grad = False\n\n# Load QA Datasets\ndataset_1 = load_dataset(\"Amod/mental_health_counseling_conversations\")  # Replace with the first QA dataset\ndataset_2 = load_dataset(\"nbertagnolli/counsel-chat\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:40.941351Z","iopub.execute_input":"2024-11-13T18:22:40.941703Z","iopub.status.idle":"2024-11-13T18:22:46.793564Z","shell.execute_reply.started":"2024-11-13T18:22:40.941667Z","shell.execute_reply":"2024-11-13T18:22:46.792837Z"}},"outputs":[{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"trainable_params = []\ncnt = 0\nfor name, param in model.named_parameters():\n    cnt+=1\n    if param.requires_grad:\n        trainable_params.append(name)\n        # print(f\"Parameter {name} will be trained.\")\n\nprint(f\"\\nTotal number of parameters: {cnt} , trainable parameters: {len(trainable_params)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:46.796780Z","iopub.execute_input":"2024-11-13T18:22:46.797065Z","iopub.status.idle":"2024-11-13T18:22:46.806530Z","shell.execute_reply.started":"2024-11-13T18:22:46.797035Z","shell.execute_reply":"2024-11-13T18:22:46.805650Z"}},"outputs":[{"name":"stdout","text":"\nTotal number of parameters: 392 , trainable parameters: 104\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# def preprocess_data_mental_health(examples):\n#     # Extract the \"context\" and \"response\" fields, stripping any extra whitespace\n#     contexts = [c.strip() for c in examples[\"Context\"]]\n#     responses = [r.strip() for r in examples[\"Response\"]]\n\n#     # Concatenate context and response for text generation\n#     inputs = [f\"{context}\\n{response}\" for context, response in zip(contexts, responses)]\n    \n#     # Tokenize the concatenated input\n#     tokenized_inputs = tokenizer(\n#         inputs,\n#         truncation=True,\n#         padding=\"max_length\",  # Adjust as needed\n#         max_length=512  # Set to your model's max input length\n#     )\n\n#     # Return tokenized inputs with labels (for text generation, labels are the same as input IDs)\n#     tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n\n#     return tokenized_inputs\n \n\n\n#\ndef preprocess_data_mental_health(examples):\n    # Concatenate Context and Response\n    inputs = [context + response for context, response in zip(examples[\"Context\"], examples[\"Response\"])]\n    \n    # Tokenize the inputs\n    model_inputs = tokenizer(\n        inputs, \n        max_length=128, \n        truncation=True, \n        padding=\"max_length\"  # or \"longest\" or \"do_not_pad\", depending on your needs\n    )\n    \n    # Prepare labels (for causal LM, labels are usually the same as input_ids)\n    labels = model_inputs[\"input_ids\"].copy()\n    \n    # Optionally, you can mask the padding tokens in the labels\n    # Replace padding token id with -100 to ignore in loss calculation\n    labels = [\n        [(label if label != tokenizer.pad_token_id else -100) for label in labels_example]\n        for labels_example in labels\n    ]\n    \n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\ntokenized_dataset_1 = dataset_1.map(preprocess_data_mental_health, batched=True)\ntokenized_dataset_1.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ndataset = tokenized_dataset_1[\"train\"].train_test_split(test_size=0.2)  # 80% train, 20% validation\ntokenized_dataset_1 = DatasetDict({\n    \"train\": dataset[\"train\"],\n    \"validation\": dataset[\"test\"]\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:46.807755Z","iopub.execute_input":"2024-11-13T18:22:46.808139Z","iopub.status.idle":"2024-11-13T18:22:47.351363Z","shell.execute_reply.started":"2024-11-13T18:22:46.808096Z","shell.execute_reply":"2024-11-13T18:22:47.350362Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenized_dataset_1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:47.352711Z","iopub.execute_input":"2024-11-13T18:22:47.353415Z","iopub.status.idle":"2024-11-13T18:22:47.360458Z","shell.execute_reply.started":"2024-11-13T18:22:47.353366Z","shell.execute_reply":"2024-11-13T18:22:47.359506Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Context', 'Response', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2809\n    })\n    validation: Dataset({\n        features: ['Context', 'Response', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 703\n    })\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"def preprocess_data_counsel_chat(examples):\n    # Extract the \"context\" and \"response\" fields, stripping any extra whitespace\n    contexts = [c.strip() if c is not None else \"\" for c in examples[\"questionText\"]]\n    responses = [r.strip() if r is not None else \"\" for r in examples[\"answerText\"]]\n\n    # Concatenate context and response for text generation\n    inputs = [f\"{context}\\n{response}\" for context, response in zip(contexts, responses)]\n    \n    # Tokenize the concatenated input\n    tokenized_inputs = tokenizer(\n        inputs,\n        truncation=True,\n        padding=\"max_length\",  # Adjust as needed\n        max_length=128  # Set to your model's max input length\n    )\n\n    # Return tokenized inputs with labels (for text generation, labels are the same as input IDs)\n    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n\n    return tokenized_inputs\n\n# Tokenize datasets with respective preprocessing functions\ntokenized_dataset_2 = dataset_2.map(preprocess_data_counsel_chat, batched=True)\ntokenized_dataset_2.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ndataset = tokenized_dataset_2[\"train\"].train_test_split(test_size=0.2)  # 80% train, 20% validation\ntokenized_dataset_2 = DatasetDict({\n    \"train\": dataset[\"train\"],\n    \"validation\": dataset[\"test\"]\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:47.361669Z","iopub.execute_input":"2024-11-13T18:22:47.361949Z","iopub.status.idle":"2024-11-13T18:22:47.878918Z","shell.execute_reply.started":"2024-11-13T18:22:47.361918Z","shell.execute_reply":"2024-11-13T18:22:47.877927Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"tokenized_dataset_2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:47.880270Z","iopub.execute_input":"2024-11-13T18:22:47.881006Z","iopub.status.idle":"2024-11-13T18:22:47.887139Z","shell.execute_reply.started":"2024-11-13T18:22:47.880958Z","shell.execute_reply":"2024-11-13T18:22:47.886147Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['questionID', 'questionTitle', 'questionText', 'questionLink', 'topic', 'therapistInfo', 'therapistURL', 'answerText', 'upvotes', 'views', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 2220\n    })\n    validation: Dataset({\n        features: ['questionID', 'questionTitle', 'questionText', 'questionLink', 'topic', 'therapistInfo', 'therapistURL', 'answerText', 'upvotes', 'views', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 555\n    })\n})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Load Metrics\nmetric_em = evaluate.load(\"exact_match\")\nmetric_f1 = evaluate.load(\"f1\")\n\ndef compute_metrics(predictions, references):\n    preds = np.argmax(predictions, axis=-1)\n    em_score = metric_em.compute(predictions=preds, references=references)\n    f1_score = metric_f1.compute(predictions=preds, references=references)\n    return {\"exact_match\": em_score, \"f1\": f1_score}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:47.888339Z","iopub.execute_input":"2024-11-13T18:22:47.888709Z","iopub.status.idle":"2024-11-13T18:22:50.159374Z","shell.execute_reply.started":"2024-11-13T18:22:47.888675Z","shell.execute_reply":"2024-11-13T18:22:50.158397Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class EWC:\n    def __init__(self, model, dataset, fisher_multiplier=0.5):\n        self.model = model\n        self.fisher_multiplier = fisher_multiplier\n        # Save only parameters that require gradients\n        self.params = {n: p.clone() for n, p in model.named_parameters() if p.requires_grad}\n        self.fisher = self.compute_fisher(dataset)\n\n    def compute_fisher(self, dataset):\n        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters() if p.requires_grad}\n        device = next(self.model.parameters()).device  # Get model device\n        dataloader = DataLoader(dataset, batch_size=4)  # Adjust batch size as needed\n\n        for batch in dataloader:\n            self.model.zero_grad()\n\n            # Move tensors to the device\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            loss = outputs.loss\n            loss.backward()\n\n            for n, p in self.model.named_parameters():\n                if p.requires_grad and p.grad is not None:\n                    fisher[n] += p.grad.data ** 2  # Use .data to avoid issues with autograd\n\n        # Normalize the Fisher information by the number of samples\n        num_batches = len(dataloader)\n        for n in fisher:\n            fisher[n] /= num_batches\n        return fisher\n\n    def penalty(self, model):\n        loss = 0.0\n        for n, p in model.named_parameters():\n            if p.requires_grad:\n                loss += (self.fisher_multiplier * self.fisher[n] * (p - self.params[n]) ** 2).sum()\n        return loss\n        \n\n# Training Loop with EWC\nclass EWCTrainer(Trainer):\n    def __init__(self, *args, ewc=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ewc = ewc\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        outputs = model(**inputs)\n        loss = outputs.loss\n        if self.ewc:\n            loss += self.ewc.penalty(model)\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:45:53.402641Z","iopub.execute_input":"2024-11-13T18:45:53.403370Z","iopub.status.idle":"2024-11-13T18:45:53.418621Z","shell.execute_reply.started":"2024-11-13T18:45:53.403325Z","shell.execute_reply":"2024-11-13T18:45:53.417776Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine-Tune on Dataset 1\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"no\",\n    learning_rate=2e-4,\n    per_device_train_batch_size = 2,\n    # per_device_eval_batch_size = 2,\n    gradient_checkpointing = True,\n    max_grad_norm = 0.3,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    fp16=False,\n    bf16 = False,\n    num_train_epochs=1,\n    weight_decay=0.001,\n    lr_scheduler_type = \"constant\",\n    max_steps = -1,\n    warmup_ratio = 0.03,\n    group_by_length = True,\n    save_steps = 25,\n    logging_steps = 25,\n\n)\n\ntrainer_1 = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset_1[\"train\"],\n    # eval_dataset=tokenized_dataset_1[\"validation\"],\n    compute_metrics=compute_metrics,\n)\n\ntrainer_1.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:22:50.178352Z","iopub.execute_input":"2024-11-13T18:22:50.178733Z","iopub.status.idle":"2024-11-13T18:43:47.387454Z","shell.execute_reply.started":"2024-11-13T18:22:50.178691Z","shell.execute_reply":"2024-11-13T18:43:47.386326Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeeps2657\u001b[0m (\u001b[33msaideep\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112638622216764, max=1.0â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56b03c8048124630bfa6678ec1db81a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241113_182254-16oa0mjf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/saideep/huggingface/runs/16oa0mjf' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/saideep/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/saideep/huggingface' target=\"_blank\">https://wandb.ai/saideep/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/saideep/huggingface/runs/16oa0mjf' target=\"_blank\">https://wandb.ai/saideep/huggingface/runs/16oa0mjf</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1405' max='1405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1405/1405 20:48, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.783300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.551300</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>2.456900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.378100</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>2.526600</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.433400</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>2.331100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.356400</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>2.431500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.373100</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>2.470900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.371800</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>2.377700</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.303000</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>2.397400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.293200</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>2.284800</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.196900</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>2.358000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.310200</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>2.212600</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>2.142700</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>2.290800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.234100</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>2.255500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>2.207900</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>2.246500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.260800</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>2.333300</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>2.103500</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>2.161700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.046400</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>2.197800</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>2.267600</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>2.138400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.079000</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>2.258200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>2.120600</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>2.212300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.108100</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>2.074100</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>2.107700</td>\n    </tr>\n    <tr>\n      <td>1075</td>\n      <td>2.173800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.187800</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>2.150100</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>2.081800</td>\n    </tr>\n    <tr>\n      <td>1175</td>\n      <td>2.090100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.114300</td>\n    </tr>\n    <tr>\n      <td>1225</td>\n      <td>2.062200</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>2.138200</td>\n    </tr>\n    <tr>\n      <td>1275</td>\n      <td>2.090700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.956900</td>\n    </tr>\n    <tr>\n      <td>1325</td>\n      <td>2.089900</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>1.891000</td>\n    </tr>\n    <tr>\n      <td>1375</td>\n      <td>2.195400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.062300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1405, training_loss=2.2374893907974625, metrics={'train_runtime': 1254.6654, 'train_samples_per_second': 2.239, 'train_steps_per_second': 1.12, 'total_flos': 4395086145847296.0, 'train_loss': 2.2374893907974625, 'epoch': 1.0})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Save EWC Fisher information after fine-tuning on Dataset 1\newc = EWC(model, tokenized_dataset_1[\"train\"])\n\n# Fine-Tune on Dataset 2 with EWC applied\ntraining_args_2 = TrainingArguments(\n    output_dir=\"./results_with_ewc\",\n    evaluation_strategy=\"no\",\n    learning_rate=2e-4,\n    per_device_train_batch_size = 4,\n    # per_device_eval_batch_size = 4,\n    gradient_checkpointing = True,\n    max_grad_norm = 0.3,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    fp16=False,\n    bf16 = False,\n    num_train_epochs=1,\n    weight_decay=0.001,\n    lr_scheduler_type = \"constant\",\n    max_steps = -1,\n    warmup_ratio = 0.03,\n    group_by_length = True,\n    save_steps = 25,\n    logging_steps = 25,\n)\n\ntrainer_2 = EWCTrainer(\n    model=model,\n    args=training_args_2,\n    train_dataset=tokenized_dataset_2[\"train\"],\n    # eval_dataset=tokenized_dataset_2[\"validation\"],\n    ewc=ewc,\n    compute_metrics=compute_metrics,\n)\n\ntrainer_2.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T18:46:00.428505Z","iopub.execute_input":"2024-11-13T18:46:00.429187Z","iopub.status.idle":"2024-11-13T19:15:04.900312Z","shell.execute_reply.started":"2024-11-13T18:46:00.429144Z","shell.execute_reply":"2024-11-13T19:15:04.899400Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='555' max='555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [555/555 13:06, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.015300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.947000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.808100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.842600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.927000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.770100</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.840700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.698800</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.861000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.775500</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.889400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.928400</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.825200</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.850200</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.879200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.755600</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.879500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.722000</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.654200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.843600</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>1.833400</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.957300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=555, training_loss=1.837767592421523, metrics={'train_runtime': 788.1726, 'train_samples_per_second': 2.817, 'train_steps_per_second': 0.704, 'total_flos': 3473510588743680.0, 'train_loss': 1.837767592421523, 'epoch': 1.0})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"model.save_pretrained(\"./fine_tuned_lora_model\")\ntokenizer.save_pretrained(\"./fine_tuned_lora_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:15:11.363350Z","iopub.execute_input":"2024-11-13T19:15:11.363770Z","iopub.status.idle":"2024-11-13T19:15:12.513594Z","shell.execute_reply.started":"2024-11-13T19:15:11.363731Z","shell.execute_reply":"2024-11-13T19:15:12.512651Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('./fine_tuned_lora_model/tokenizer_config.json',\n './fine_tuned_lora_model/special_tokens_map.json',\n './fine_tuned_lora_model/tokenizer.model',\n './fine_tuned_lora_model/added_tokens.json',\n './fine_tuned_lora_model/tokenizer.json')"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Push the LoRA model to Hugging Face Hub\nmodel.push_to_hub(\"Saideep14/test-qa-lora\")\ntokenizer.push_to_hub(\"Saideep14/test-qa-lora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T19:15:12.515249Z","iopub.execute_input":"2024-11-13T19:15:12.515560Z","iopub.status.idle":"2024-11-13T19:15:31.048123Z","shell.execute_reply.started":"2024-11-13T19:15:12.515527Z","shell.execute_reply":"2024-11-13T19:15:31.047224Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83b742008f944d0b94a5d26c717c55cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/51.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72d445d4070449ae83e883a00c7f928b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0cb28e6b181420080ab6a8c67e834c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a7e279527864356ae28758d29d2d622"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb8a59574cd0474091b2778bd50eb98a"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Saideep14/test-qa-lora/commit/471e98b0ae11e36bdf034f1ee4a82d88313170eb', commit_message='Upload tokenizer', commit_description='', oid='471e98b0ae11e36bdf034f1ee4a82d88313170eb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Saideep14/test-qa-lora', endpoint='https://huggingface.co', repo_type='model', repo_id='Saideep14/test-qa-lora'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"## Testing","metadata":{}}]}